Kubernetes - Autoscaling - auto adjust resources allocated to applications

Autoscaling in kubernetes is the ability to automatically adjust the 
resources allocated to applications based on their current usage and demand.

Several autoscaling features in k8s

1. Horizontal Pod Autoscaler (HPA)
2. Vertical Pod Autoscaler (VPA)
3. Cluster Autoscaler (CA)
4. Pod Priority and Preemption
5. Resource Quotas and limits

Why autoscaling feature is needed in kubernetes ?

Most application workloads have daily, weekly, and seasonal rhythms driven by 
user activity. This variability can either cause application performance to 
degrade due to resource constraints, or result in unnecessary resource wastage 
due to overprovisioning.

Therefore k8s autoscaling feature is used to
1. automatically adjust the resources allocated to applications based on their 
   current usage and demand
2. improve application performance.
3. reduce operational overhead.
4. manage resources efficiently.

1. HPA 
- automatically scales (up or down) the number of pod replicas in a Deployment, 
  ReplicaSet, or StatefulSet on the basis of observed CPU usage or custom metrics
  and target thresholds.
- ensures that there are enough pod replicas to handle the current workload 
  and maintain optimal performance.

2. VPA
- automatically adjusts the CPU and memory resource requests of pods based on their
  resource usage patterns
- optimizes resource allocation by adjusting the resource requests to match the actual
  resource consumption of the pods
- help improve resource utilization and reduce wastage by right-sizing pods dynamically

3. CA
- automatically adjusts the size of the Kubernetes cluster by adding or removing nodes
  based on the resource demands of the pods scheduled on it.
- ensures enough nodes are available to meet the current workload demand.
- integrates with cloud providers' APIs to provision and terminate nodes dynamically.

4. Pod Priority and Preemption:
- to prioritize critical pods over non-critical ones during resource contention.
- help maintain service levels for critical applications during periods of high demand.

5. Resource Quotas and limits
- k8s allows specifying res quotas and limits at namespace level (for each namespace)
- Resource quotas - specifies max amount of CPU, memory and other resources that can be 
  consumed within a namespace
- Resource limits - specifies max amount of CPU and memory resources that can be
  allocated to individual pods within a namespace.

What is Cluster Autoscaler? How is it configured in  kubernetes?

Cluster Autoscaler is a Kubernetes component that automatically adjusts the size of a Kubernetes 
cluster to meet the demands of its workload. It adds or removes nodes based on the resource 
utilization of the cluster, ensuring that there are enough resources available to schedule pods 
and maintain optimal performance.

Here's how Cluster Autoscaler works:

Monitoring Resource Utilization: Cluster Autoscaler continuously monitors the resource utilization
of the nodes in the cluster. It looks at metrics such as CPU and memory usage to determine if the 
cluster has enough capacity to accommodate the pods scheduled on it.

Scaling Decisions: If Cluster Autoscaler detects that there are pods pending due to insufficient 
resources, it will trigger a scaling event. This can involve either adding new nodes to the cluster 
or removing existing nodes that are underutilized.

Node Provisioning and Termination: When scaling up, Cluster Autoscaler communicates with the cloud 
provider's API to provision new nodes and add them to the cluster. Conversely, when scaling down, 
it identifies nodes that are not needed and cordons and drains them to safely evict the pods before 
terminating the nodes.

Configurable Policies: Cluster Autoscaler can be configured with policies to control its behavior, 
such as minimum and maximum node counts, node instance types, and node groups. These policies ensure 
that the cluster scales within specified boundaries and adheres to any constraints or requirements.

Integration with Kubernetes Scheduler: Cluster Autoscaler works seamlessly with the Kubernetes 
scheduler to ensure that pods are distributed evenly across nodes and that scheduling decisions 
consider the current cluster capacity.

Configuring Cluster Autoscaler involves setting up parameters that define its behavior and integration 
with the cloud provider. Here's a basic example of how you might configure Cluster Autoscaler for a 
Kubernetes cluster on AWS:

1. Deployment:

apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      containers:
      - image: k8s.gcr.io/cluster-autoscaler:v1.21.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/example-cluster
        - --balance-similar-node-groups

RBAC Configuration: Ensure that Cluster Autoscaler has the necessary permissions to interact with the 
Kubernetes API server.

Cloud Provider Integration: Configure Cluster Autoscaler to work with your cloud provider (e.g., AWS, 
GCP, Azure) by specifying the appropriate flags (e.g., --cloud-provider=aws).

Node Group Auto-Discovery: Specify the node groups that Cluster Autoscaler should manage. This can be 
done using tags or other identifiers associated with the nodes.

Once configured, Cluster Autoscaler will automatically adjust the size of the Kubernetes cluster based 
on workload demands, ensuring efficient resource utilization and optimal performance.

How is Cluster Autoscaler different from AWS Auto scaling group?

Cluster Autoscaler and AWS Auto Scaling groups are related but serve different purposes within the 
context of managing resources in a Kubernetes cluster on AWS.

Here's a breakdown of the key differences:

Scope:

Cluster Autoscaler: Operates at the level of the Kubernetes cluster, managing the number of nodes in the 
cluster based on the resource demands of the pods scheduled on it.
AWS Auto Scaling group: Operates at the level of EC2 instances in AWS, managing the number of EC2 instances
 based on predefined scaling policies, such as CPU utilization, network traffic, or custom metrics.
Resource Management:

Cluster Autoscaler: Focuses on managing the Kubernetes cluster nodes, ensuring that there are enough 
resources available to schedule pods and maintain optimal performance.
AWS Auto Scaling group: Manages the scaling of EC2 instances in response to changing demand, ensuring 
that the application hosted on those instances remains available and responsive.
Integration:

Cluster Autoscaler: Integrates directly with the Kubernetes control plane and interacts with the Kubernetes 
API server to monitor resource utilization and make scaling decisions.
AWS Auto Scaling group: Integrates with AWS services, such as EC2 and Elastic Load Balancing, to provision 
and manage EC2 instances based on scaling policies defined in the Auto Scaling group configuration.

Scaling Triggers:

Cluster Autoscaler: Scales the Kubernetes cluster based on factors such as pod scheduling failures, 
pod eviction due to resource constraints, or node underutilization.
AWS Auto Scaling group: Scales EC2 instances based on predefined scaling policies, which can include
metrics like CPU utilization, network traffic, or custom application metrics.

Granularity:

Cluster Autoscaler: Operates at the level of individual nodes in the Kubernetes cluster, allowing for
granular control over the cluster's resource allocation.
AWS Auto Scaling group: Operates at the level of EC2 instances, which may host multiple pods or services, 
providing broader resource management capabilities.

In summary, while both Cluster Autoscaler and AWS Auto Scaling groups are used to manage resources 
dynamically, they operate at different levels of abstraction and serve different purposes within the 
Kubernetes-on-AWS ecosystem. Cluster Autoscaler focuses on managing the Kubernetes cluster nodes to 
ensure optimal resource utilization, while AWS Auto Scaling groups manage the scaling of EC2 instances 
based on predefined policies to maintain application availability and performance.
