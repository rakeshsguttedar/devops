Kubernetes Deep Dive - B1 - Part 2 

Kubernetes provides a basic resource called Pod.
 
A pod is the smallest deployable unit in Kubernetes which is actually a wrapper
around containers. A pod can have one or more containers and you can pass 
different configurations to the container(s) using the podâ€™s configuration 
e.g. passing environment variables, mounting volumes, having health checks, etc.

A pod is managed with the help of application manifest yaml file.

Examples of pod having more than 1 container: 
-> ephemeral containers
-> init containers
-> sidecar containers

Ephemeral containers:  a special type of container that runs temporarily in an 
existing Pod to accomplish user-initiated actions such as troubleshooting. You 
use ephemeral containers to inspect services rather than to build applications.
Example: troubleshooting of issues in distroless image containers

Init container: specialized containers that run before app containers in a Pod.
Init containers can contain utilities or setup scripts not present in an app image
Init containers run and complete their tasks before the main application container
starts. Unlike sidecar containers, init containers are not continuously running 
alongside the main containers. Init containers run to completion sequentially, and
the main container does not start until all the init containers have successfully 
completed. Init containers do not support lifecycle, livenessProbe, readinessProbe,
or startupProbe whereas sidecar containers support all these probes to control 
their lifecycle. Init containers share the same resources (CPU, memory, network) with
the main application containers but do not interact directly with them. They can, 
however, use shared volumes for data exchange.

Sidecar container: enhance or to extend the functionality of the main
application container by providing additional services, or functionality
such as logging, monitoring, security, or data synchronization, 
without directly altering the primary application code.

I. Architecture of k8s cluster:

Master Node - a VM that manages Worker Nodes: WN-1 and WN-2
Processes running in master node:
1. API server - Most imp component - a cluster gateway - all requests to k8s 
   resources first comes to the API server.
2. Controller Manager: CM is responsible for conveying information regarding 
   state changes - make changes attempting to move the current state towards
   the desired state i.e. If a pod has stopped or crashed, a new pod is created.
3. Scheduler: responsible for scheduling of resources or pods on a worker node
   based on the CPU or memory resource available on a node.
4. Etcd - cluster brain - stores info about the k8s cluster in key-value pairs
   Scheduler and Controller Manager uses etcd information.

Worker Nodes: WN-1 and WN-2 - each is a VM having 128 GB of RAM and 1TB storage.
Three processes running in a worker node:
1. Container runtime - containerd - resonsible for running the container inside pod.
2. kubelet - responsible for creation of pod
3. kubeproxy - manages the networking aspect

k8s resource request flow:
User -> API server -> Scheduler -> kubelet -> containerd
Controller Manager (CM) monitors the state, if pod crashes/stops, informs the 
scheduler ->kubelet -> containerd

How will the user create an application in kubernetes?
User creates yam files: deployment.yml and Service.yml
These yaml manifests are shared with API server in Master node. API server forwards
it to the Scheduler. Scheduler then forwards it to the kubelet in Worker node. 
kubelet creates the pods and container runtime (containerd or CRI-O) will run the 
container inside the pod.

II. Kubernetes Installation

Kubernetes cluster setup on a VM using AWS EC2 instances:

AWS Console -> Create 2 EC2 instances of type t2.medium with 15 GB storage. 
One instance as master node and other will be running a worker node.
Below are the Ports to be opened in Inbound-rules of Secuirty Group attached
to these EC2 instances.

1. 6443
2. 30000-32768

Installing Kubernetes Master Nodes and Worker nodes in EC2 instances:

Step 1: Configuration changes in both Master node and Worker node:

1. Update the package lists for all repositories:
[configured in /etc/apt/sources.list and in the /etc/apt/sources.list.d/ directory.]
sudo apt update -y
sudo apt-get update

2. Install docker and restart docker service:
sudo apt-get install docker.io -y
sudo service docker restart
sudo chmod 666 /var/run/docker.sock

3. Install Dependencies for Kubernetes:

For V1.28 
sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
sudo mkdir -p -m 755 /etc/apt/keyrings

3. Add GPG key for downloading Kubernetes package from GCP:

become root user. 

sudo su 

sudo curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -
For V1.28 
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

It fetches the GPG key for the Kubernetes packages and adds it to the keyring.

4. Add Kubernetes repository:
echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" >/etc/apt/sources.list.d/kubernetes.list

For V1.28 
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

Adds the Kubernetes repository to the system's package sources.

exit 

to become ubuntu user

5. Update the package list again:
sudo apt-get update

6. Install specific versions of Kubeadm, Kubectl, and Kubelet:
sudo apt install kubeadm=1.20.0-00 kubectl=1.20.0-00 kubelet=1.20.0-00 -y
For V1.28
sudo apt install -y kubeadm=1.28.1-1.1 kubelet=1.28.1-1.1 kubectl=1.28.1-1.1

Installs specific versions of Kubeadm, Kubectl, and Kubelet

Step 2: Configuration changes in Master Node:

7. Initialize the Kubernetes cluster with a specified pod network CIDR:
sudo kubeadm init --pod-network-cidr=192.168.0.0/16

Initializes the Kubernetes master node, specifying the pod network CIDR for 
communication between pods.

After we run the kubeadm init command, kubeadm will generate a 
kubeadm join command.

This command has to be run in the one or more worker nodes to join them to this master node.

[ Example for reference only:
sudo kubeadm join 172.31.16.59:6443 --token cskhjwq.3432dfvdg5545tbfg25  --discovery-token-ca-cert-hash sha256:123456qwertyuiop423423423234255456567867867ddgnvnghhkhh ]

8. Create a directory for kubeconfig and copy the admin.conf file:
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

Creates a directory for storing kubeconfig files and copies the admin.conf file
to the appropriate location.

9. Set ownership for the kubeconfig file:
sudo chown $(id -u):$(id -g) $HOME/.kube/config

Ensures that the user has ownership of the kubeconfig file.

10. Apply Calico network plugin:
kubectl apply -f https://docs.projectcalico.org/v3.20/manifests/calico.yaml

Applies the Calico network plugin manifest to enable networking within the Kubernetes cluster.

11. Apply Ingress-Nginx controller:
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.49.0/deploy/static/provider/baremetal/deploy.yaml

Applies the Ingress-Nginx controller manifest for handling Ingress resources in the cluster.
i.e. steps 10 and 11 installs Custom Resource Definitions (CRDs) for ingress controller and Calico CNI.

12. kubeval - To validate the yaml manifests before applying
Installation:
wget https://github.com/instrumenta/kubeval/releases/latest/download/kubeval-linux-amd64.tar.gz
tar xf kubeval-linux-amd64.tar.gz
sudo cp kubeval /usr/local/bin

Kubeadm:
1. initialize a control plane node; installs and configures: API server, Controller Manager and Scheduler.
2. TLS Certifcate generation - needed for securing the communication between k8s components through 
   Authentication and encryption.
3. kubeconfig generation for both control plane nodes and worker nodes
4. Joining Worker nodes - to join additional worker nodes to the cluster. 
5. Networking - Kubeadm can help configure the chosen CNI plugin to work with the cluster.
6. Add-ons or components installation :  can optionally install additional add-ons or components
   Kubernetes DNS service (CoreDNS), Kubernetes Dashboard, or monitoring tools like Prometheus and Grafana.

Few Initial commands that are run in the k8s master node:

kubectl get nodes
It will show the number of nodes in k8s cluster. In our case, there are two nodes,
1 master node and 1 worker node.

kubectl get pods
No resouces are found in default namespace.
The initial namespace in k8s cluster is called a default namespace.

NAMESPACES
what are namespaces?
Namespaces ns - logical isolation for k8s objects. 
It is mainly used for isolating different projects within a k8s cluster.
It is like a seperate box for each project. 
To list all namespaces in k8s cluster:
kubectl get namespace
kubectl get ns # namespace short form is ns

To create a namespace:
kubectl create namespace mynamespace

Note: In openshift, namespace is called as projects.
openshift uses oc command in place of kubectl command.

CREATING DEPLOYMENT AND PODS:

deployment
replicaset
pod
container

Deployment creates the replicaset which will create the replication controller

kubectl create deployment nginx123 --image=nginx 
# pulls latest image from the dockerhub registry
kubectl get deployment
It shows the deployment created

kubectl get pod
It shows all the pods in the default namespace

kubectl get pod -o wide
It shows all the pods with detailed imformation such as IP address,
NODE, NOMINATED NODE and READINESS GATES

kubectl get replicaset
It shows all the replicaset created for this deployment

kubectl delete pod pod-name
# will create a new pod and deletes the pod pod-name
This behaviour is because of replicaset which initiates the
creation of new pod to maintain the desired state and thereby
does auto-healing.

Edit deployment to increase the number of replicas:

kuebctl edit deployment nginx123
We will see a deployment yaml manifest file is opened and we edit the line with
replicas: 1 to make it 2. 

After this command, one more pod will be created. This behaviour is because of
replicaset which initiates the creation of new pod to maintain the desired state 
and thereby does scaling. The state is changed because we edited the deployment
and changed the replicas.

kubectl delete deployment nginx123
# will delete the deployment

All the pods will be terminated before the deployment is deleted.