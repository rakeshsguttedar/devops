Kubernetes Taints and Tolerations

Node affinity is a property of Pods that attracts them to a set of 
nodes (either as a preference or a hard requirement). Taints are the 
opposite -- they allow a node to repel a set of pods.

A taint allows a node to refuse pod to be scheduled unless that pod 
has a matching toleration.

Refer the Kubernetes documnentation for more detailed description

https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/

Taints and Tolerations are the mechanisms in Kubernetes to configure
scheduling of pods on a Node based on the attributes set on the Node.

Setting of an attribute on a Node is called tainting of node. And
the attribute that is set on the node is called Taint. 

If the same attribute is configured on the Pod, it is called Toleration.
Hence Taints are set on Nodes and Tolerations are set on the pods.

You apply taints to a node through the node specification (NodeSpec) and 
apply tolerations to a pod through the pod specification (PodSpec). A taint 
on a node instructs the node to repel all pods that do not tolerate the taint.

Taints and tolerations are mechanisms used to control which nodes a pod
can be scheduled on. They are important for ensuring that certain pods
are only deployed on specific nodes or types of nodes.

A node is tainted to ensure that no pod is scheduled on that node 
unless a pod is configured to tolerate this node by using the same 
attribute that is set on the Node. A Pod is said to be tolerant to a 
tainted node if the same taint is configured as tolerations on that Pod.

Attribute or Taint - 
A taint is a key-value pair that is applied to a node. It prevents pods 
from being scheduled onto a node unless the pod has a corresponding 
toleration.

Taints are typically used to mark nodes as unsuitable for certain types of
workloads, such as reserving nodes for specific tasks or isolating nodes 
due to hardware limitations.

An attribute or Taint is in the form

      key=value:taint-effect 

where 
key=value is the property and its value 
taint-effect is the behaviour of the Pods after the Node is tainted.

The possible values for taint-effect are 

NoSchedule
NoSchedule ensures no new pods are scheduled on this tainted node
unless they have a matching toleration. But already scheduled pods
continue to run unless evicted because of other reasons.

PreferNoSchedule
PreferNoSchedule can allow scheduling of pods without the tolerations 
for key=value on that node. This effect is used when you want to 
discourage scheduling pods onto specific nodes but allow it if necessary.
PreferNoSchedule is a "preference" or "soft" version of NoSchedule. The
control plane will try to avoid placing a Pod that does not tolerate the 
taint on the node, but it is not guaranteed.

NoExecute
NoExecute will evict the Pods without tolerations if they are already
Sceduled on that Node. This effect ensures that only pods with tolerations
for the taint are running on the node.
This affects pods that are already running on the node as follows
Pods that do not tolerate the taint are evicted immediately
Pods that tolerate the taint without specifying tolerationSeconds in their 
toleration specification remain bound forever.
Pods that tolerate the taint with a specified tolerationSeconds remain bound
for the specified amount of time. After that time elapses, the node lifecycle
controller evicts the Pods from the node.

You can put multiple taints on the same node and multiple tolerations on the
same pod.

Let us explain using an example.

We can place a taint on a particular node by the command

kubectl taint nodes node1  key1=value1:NoSchedule

The taint has key key1, value value1, and taint effect NoSchedule. 
This means that no pod will be able to schedule onto node1 unless 
it has a matching toleration.

To remove the taint added by the command above, you can run

kubectl taint nodes node1 key1=value1:NoSchedule-

For example
kubectl taint nodes node1 app=backend:NoSchedule

In this example, we're tainting node1 with a key-value pair app=backend and
the taint effect NoSchedule. This means that pods without a toleration for 
app=backend will not be scheduled on node1.

Tolerations
A toleration is a part of a pod's specification that allows the pod to
be scheduled onto a node with a matching taint. It effectively tells the
scheduler that the pod is "tolerant" of the taint on the node.

Example of adding a toleration to a pod

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: my-image
  tolerations:
  - key: "app"
    operator: "Equal"
    value: "backend"
    effect: "NoSchedule"

In this example, the pod has a toleration for the taint with key app, 
value backend, and taint effect NoSchedule. This means that the pod 
can be scheduled on a node with the taint app=backend:NoSchedule.
The default vlaue for operator is Equal if not specified.

Taint Effects

NoSchedule - The pod will not be scheduled on the tainted node.

PreferNoSchedule - The scheduler will try to avoid placing the pod on the
tainted node, but it's not an absolute restriction.

NoExecute - If a pod is already running on a node when it gets tainted with
NoExecute, the pod will be evicted from the node.

Hands-on example of Taint and Toleration

Below is a spring boot application deployment manifest.

tolerate.yml

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spring-boot-k8s-deployment
spec:
  selector:
    matchLabels:
      app: spring-boot-k8s
  replicas: 2
  template:
    metadata:
      labels:
        app: spring-boot-k8s
    spec:
      containers:
        - name: spring-boot-k8s
          image: adijaiswal/shopping-cart:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8070
      tolerations:
        - key: app
          operator: Equal
          value: backend
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: springboot-k8ssvc
spec:
  selector:
    app: spring-boot-k8s
  ports:
    - protocol: TCP
      port: 8070
      targetPort: 8070
  type: NodePort


1. Deploy the spring boot application using
kubectl apply -f tolerate.yml

2. We can check all the nodes using the command 
kubectl get nodes

3. From this list, we can taint a specific node using

kubectl taint node node2 app=backend:NoSchedule

4. We can check the taint using

kubectl describe nodes | grep -e Name: -e Taint

5. we can check on which node the pods are scheduled using the command

kubectl get pods -o wide

Result - 
You can see that the 2 pods are scheduled on node2 because
the toleration on these pods match the taint on node2.

A few of typical scenrios where Taints and Tolerations are used 

Dedicating a node for a user
Binding a user to a node
Dedicating nodes with special hardware - like GPU